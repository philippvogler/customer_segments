{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Customer Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project you, will analyze a dataset containing annual spending amounts for internal structure, to understand the variation in the different types of customers that a wholesale distributor interacts with.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Run each code block below by pressing **Shift+Enter**, making sure to implement any steps marked with a TODO.\n",
    "- Answer each question in the space provided by editing the blocks labeled \"Answer:\".\n",
    "- When you are done, submit the completed notebook (.ipynb) with all code blocks executed, as well as a .pdf version (File > Download as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries: NumPy, pandas, matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Read dataset\n",
    "data = pd.read_csv(\"wholesale-customers.csv\")\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "print data.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** In this section you will be using PCA and ICA to start to understand the structure of the data. Before doing any computations, what do you think will show up in your computations? List one or two ideas for what might show up as the first PCA dimensions, or what type of vectors will show up as ICA dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    ">**PCA**  \n",
    "tries to find correlations by looking for maximum variance. So the first dimensions for the PCs are, what has the largest range of values in the data set.  \n",
    "Possible first dimensions:\n",
    "- the amount in witch a certain category is replenished\n",
    "- the size of the customers business\n",
    "\n",
    ">**ICA**  \n",
    "tries to transform the feature space towards maximal independance. So the first dimension will contain the most common information in the data set. And the following dimensions will consist of the least common information.  \n",
    "Possible type of vectors dimensions: \n",
    "- the average replenishment profile of a category\n",
    "- different replenishment profiles\n",
    "\n",
    "source: https://www.udacity.com/course/viewer#!/c-ud727-nd/l-5453051650/m-661438547"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Apply PCA with the same number of dimensions as variables in the dataset\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = data.shape[1])\n",
    "pca.fit(data)\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n",
    "\n",
    "# Print the components and the amount of variance in the data contained in each dimension\n",
    "print pca.components_\n",
    "print pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** How quickly does the variance drop off by dimension? If you were to use PCA on this dataset, how many dimensions would you choose for your analysis? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCs variance plot\n",
    "\n",
    "cs = np.cumsum(pca.explained_variance_ratio_)\n",
    "#print cs\n",
    "#https://github.com/numpy/numpy/blob/v1.10.0/numpy/core/fromnumeric.py#L2038-L2106\n",
    "#http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.cumsum.html\n",
    "\n",
    "# Number array for PCs\n",
    "num_PCs = np.arange(data.shape[1])+1\n",
    "#print np.arange(data.shape[1])+1\n",
    "#http://docs.scipy.org/doc/numpy-1.10.1/user/basics.creation.html\n",
    "\n",
    "plt.plot(num_PCs, cs)\n",
    "#http://matplotlib.org/users/pyplot_tutorial.html\n",
    "\n",
    "plt.axis([1, data.shape[1], 0, 1.2])\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Cum. explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    ">The first two PCs make for more than 80% of explained variance. The third PC adds 7% to explained variance. All additional PCs add less than 5% to explained variance. \n",
    "Depending on how much I want/need to reduce my data set, I would pick the first 2 to 3 PCs. In this case, with a relatively small dataset, it is reasonable to use 3 PCs. With larger datasets, I probably go for only 2 PCs, because it is computational cheaper/faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** What do the dimensions seem to represent? How can you use this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    "> These dimensions represent the directions in R<sup>6</sup> that show the most variance i.e. the largest range of values.   \n",
    "Probably the **first dimension** is the **amount of money** that is spend on single category by a singel customer. The **second dimension** is most likely the **size of the customer** companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Fit an ICA model to the data\n",
    "# Note: Adjust the data to have center at the origin first!\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# feature centering\n",
    "scaler = preprocessing.StandardScaler(copy=True, with_mean=True, with_std=False).fit(data.astype(float))\n",
    "data_centered = scaler.transform(data)\n",
    "# http://scikit-learn.org/stable/modules/preprocessing.htmltable/auto_examples/decomposition/plot_faces_decomposition.html#example-decomposition-plot-faces-decomposition-py\n",
    "\n",
    "ica = FastICA(n_components=None, algorithm='parallel', whiten=True, fun='logcosh', fun_args=None, max_iter=200, tol=0.0001, w_init=None, random_state=None)\n",
    "ica.fit(data_centered).transform(data_centered)\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA\n",
    "\n",
    "# Print the independent components\n",
    "print ica.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** For each vector in the ICA decomposition, write a sentence or two explaining what sort of object or property it corresponds to. What could these components be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    "> The first vector is the average (standardized) replenishment profile over all categories. The second to sixth vector are (standardized) building blocks that can be lineary combined with the first vector to creat replenishment profiles for each of the six categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "In this section you will choose either K Means clustering or Gaussian Mixed Models clustering, which implements expectation-maximization. Then you will sample elements from the clusters to understand their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Cluster Type\n",
    "\n",
    "**5)** What are the advantages of using K Means clustering or Gaussian Mixture Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "> #### K Means advantages  \n",
    "> - \"scales well to large number of samples\"\n",
    "> - \"General-purpose\" clustering algorithm\n",
    "> - \"has been used (succsessfully) across a large range of application areas\"\n",
    "> - \"KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.\"\n",
    "\n",
    "> #### Gaussian Mixture Models advantages  \n",
    "> - \"incorporate information about the covariance structure of the data\"\n",
    "> - \"can also draw confidence ellipsoids for multivariate models\"\n",
    "> - \"compute the Bayesian Information Criterion to assess the number of clusters in the data\"\n",
    "> - \"it is the fastest algorithm for learning mixture models\"\n",
    "> - \"as this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply.\"\n",
    "\n",
    "source: http://scikit-learn.org/stable/modules/mixture.html#mixture, http://scikit-learn.org/stable/modules/clustering.html#k-means, https://www.udacity.com/course/viewer#!/c-ud727-nd/l-5455061279/m-638188663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** Below is some starter code to help you visualize some cluster data. The visualization is based on [this demo](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html) from the sklearn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import clustering modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: First we reduce the data to two dimensions using PCA to capture variation\n",
    "\n",
    "pca2 = PCA(n_components = 2)\n",
    "reduced_data = pca2.fit(data).transform(data)\n",
    "# http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#example-decomposition-plot-pca-vs-lda-py\n",
    "\n",
    "print reduced_data[:10]  # print upto 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement your clustering algorithm here, and fit it to the reduced data for visualization\n",
    "# The visualizer below assumes your clustering object is named 'clusters'\n",
    "\n",
    "#clf = KMeans(n_clusters = 5, init = 'k-means++', n_init = 10) #{'k-means++', 'random' or an ndarray}\n",
    "clf = GMM(n_components = 5, n_init = 10)\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GMM.html#sklearn.mixture.GMM\n",
    "\n",
    "clusters = clf.fit(reduced_data)\n",
    "\n",
    "#for KMeans\n",
    "#print clf.inertia_\n",
    "\n",
    "#for GMM\n",
    "#print clf.converged_\n",
    "\n",
    "print clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary by building a mesh grid to populate a graph.\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "hx = (x_max-x_min)/1000.\n",
    "hy = (y_max-y_min)/1000.\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Find the centroids for KMeans or the cluster means for GMM \n",
    "\n",
    "# for KMeans\n",
    "#centroids = clf.cluster_centers_\n",
    "\n",
    "# for GMM\n",
    "centroids = clf.means_\n",
    "\n",
    "print centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)** What are the central objects in each cluster? Describe them as customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    "> **blue**: huge number of small customers how are ordeirng very little  \n",
    "> **brown**: bigger customers how are not ordering very much  \n",
    "> **purple**: medium to lagre companys with limited turne-over  \n",
    "> **orange**: smaller customers howw order quit heavily  \n",
    "> **green**: companies of all size ordering a lot  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "** 8)** Which of these techniques did you feel gave you the most insight into the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    "It seems to me that GMM is fitting the data better with its elipsoid cluster boarders. This fits the distribution of the data better.  \n",
    "As I mentiond aboave I think it would make sens to use the third PCs as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**9)** How would you use that technique to help the company design new experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10)** How would you use that data to help you predict future customer needs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "You can make better predictions, if you put a new customer into the designated cluster first and make your predictions from there. Instead of predicting from the whole set of customers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
